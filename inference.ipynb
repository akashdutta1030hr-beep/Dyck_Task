{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.10.0"},
    "colab": {"provenance": []}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Dyck Task - Inference (Google Colab)\n", "\n", "Run inference with your fine-tuned model. Upload `results/` or set `MODEL_PATH` to that folder."]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": ["# Optional: !pip install -q torch transformers unsloth bitsandbytes"],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import json\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "MODEL_PATH = \"results\"\n",
        "MAX_LENGTH = 2048\n",
        "OUTPUT_PATH = \"inference_results.jsonl\"\n",
        "BRACKET_PAIRS = [(\"(\", \")\"), (\"[\", \"]\"), (\"{\", \"}\"), (\"<\", \">\"), (\"⟨\", \"⟩\"), (\"⟦\", \"⟧\"), (\"⦃\", \"⦄\"), (\"⦅\", \"⦆\")]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Loading model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_name=MODEL_PATH, max_seq_length=MAX_LENGTH, dtype=torch.float16, load_in_4bit=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"Model loaded.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sequence = \"<[<⟨{(<[((\"\n",
        "bracket_list = \", \".join(f\"{o}/{c}\" for o, c in BRACKET_PAIRS)\n",
        "prompt = f\"Bracket pairs (opening/closing): {bracket_list}. Each symbol is a single bracket (e.g. ⟦ is opening, ⟧ is closing).\\nComplete the following Dyck sequence: {sequence}\\nEnd with FINAL ANSWER: then only the closing brackets.\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "user_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "user_tokenized = tokenizer(user_text, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\", padding=True, add_special_tokens=True)\n",
        "input_ids = user_tokenized[\"input_ids\"].to(model.device)\n",
        "prompt_length = input_ids.shape[1]\n",
        "output_ids = model.generate(input_ids, max_new_tokens=1024, do_sample=True, temperature=0.1, top_p=0.9, top_k=10, repetition_penalty=1.25, pad_token_id=tokenizer.pad_token_id)\n",
        "generated_ids = output_ids[0][prompt_length:]\n",
        "response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "if \"FINAL ANSWER:\" in response:\n",
        "    response = response.split(\"FINAL ANSWER:\")[-1].strip().split(\"\\n\")[0].strip()\n",
        "print(response)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with open(OUTPUT_PATH, 'a', encoding='utf-8') as f:\n",
        "    f.write(json.dumps({\"sequence\": sequence, \"response\": response}, ensure_ascii=False) + '\\n')\n",
        "print(f\"Saved to {OUTPUT_PATH}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
