{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dyck inference (Google Colab)\n",
        "\n",
        "Run inference with the Hugging Face model **akashdutta1030/dyck-deepseek-r1-lora**. Matches `inference.py`.\n",
        "\n",
        "1. **Runtime → Change runtime type → GPU** (T4 or better).\n",
        "2. Run all cells in order.\n",
        "3. See **EXTRACTED ANSWER** for the Dyck completion (aligned with training format)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers peft accelerate torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face model (adapter)\n",
        "MODEL_ID = \"akashdutta1030/dyck-deepseek-r1-lora\"\n",
        "BASE_MODEL_ID = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "MAX_LENGTH = 2048\n",
        "\n",
        "# Inference controls (match inference.py)\n",
        "TEMPERATURE = 0.05\n",
        "MAX_NEW_TOKENS = 256\n",
        "REPETITION_PENALTY = 1.1\n",
        "TOP_P = 0.9\n",
        "EXTRACT_ANSWER = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(sequence: str) -> str:\n",
        "    \"\"\"Same format as generator._format_question (and training).\"\"\"\n",
        "    return f\"\"\"Complete the following Dyck language sequence by adding the minimal necessary closing brackets.\n",
        "\n",
        "Sequence: {sequence}\n",
        "\n",
        "Rules:\n",
        "- Add only the closing brackets needed to match all unmatched opening brackets\n",
        "- Do not add any extra bracket pairs beyond what is required\n",
        "\n",
        "Provide only the complete valid sequence.\"\"\"\n",
        "\n",
        "\n",
        "def extract_answer(response: str) -> str:\n",
        "    \"\"\"Extract Dyck completion from model output (FINAL ANSWER: or bracket-only line).\"\"\"\n",
        "    if not EXTRACT_ANSWER:\n",
        "        return response.strip()\n",
        "    if \"FINAL ANSWER:\" in response:\n",
        "        part = response.split(\"FINAL ANSWER:\")[-1].strip().split(\"\\n\")[0].strip()\n",
        "        if part:\n",
        "            return part\n",
        "    bracket_chars = set(\"()[]{}<>⟨⟩⟦⟧⦃⦄⦅⦆\")\n",
        "    for line in reversed(response.split(\"\\n\")):\n",
        "        line = line.strip()\n",
        "        if line and all(c in bracket_chars or c.isspace() for c in line):\n",
        "            return line.replace(\" \", \"\")\n",
        "    return response.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"Loading model from Hugging Face...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, MODEL_ID)\n",
        "model.eval()\n",
        "print(\"Model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input bracket sequence (change this to try other sequences)\n",
        "sequence = \"<[<⟨{(<[((\"\n",
        "\n",
        "prompt = format_prompt(sequence)\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    max_length=MAX_LENGTH,\n",
        ").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        do_sample=True,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        repetition_penalty=REPETITION_PENALTY,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "raw_response = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
        "    skip_special_tokens=True,\n",
        ").strip()\n",
        "answer = extract_answer(raw_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"INPUT SEQUENCE:\")\n",
        "print(\"=\" * 50)\n",
        "print(sequence)\n",
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"MODEL OUTPUT (raw, first 500 chars):\")\n",
        "print(\"=\" * 50)\n",
        "print(raw_response[:500] + (\"...\" if len(raw_response) > 500 else \"\"))\n",
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"EXTRACTED ANSWER (Dyck completion):\")\n",
        "print(\"=\" * 50)\n",
        "print(answer)\n",
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"Result (JSON):\")\n",
        "print(\"=\" * 50)\n",
        "result = {\"sequence\": sequence, \"response\": raw_response, \"answer\": answer}\n",
        "print(json.dumps(result, ensure_ascii=False, indent=2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
